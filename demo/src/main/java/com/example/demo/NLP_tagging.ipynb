{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDLG2qoKnYCeMvi+cd5MuK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asma71612/cognitive-decline-monitoring/blob/main/NLP_tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complex transcript below with lots of filler words, contractions, and sentences. Tokenizing was completed on each sentence rather than the entire text to improve accuracy. Had an 88% accruacy when tokenizing with filler words in, but removing the knwon ones then processing resulted in tokenization accuracy of 90%"
      ],
      "metadata": {
        "id": "Rvm0iaoDvp0I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32nU6vHAqwb9",
        "outputId": "4e6d595e-7437-4837-8048-e5dd82deadd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Uh', 'okay', 'so', 'I', 'think', 'it', '’', 's', 'a', 'kitchen', '.'], ['There', '’', 's', 'um', 'some', 'cookies', 'on', 'the', 'counter', '.'], ['I', 'don', '’', 't', 'know', ',', 'it', 'looks', 'like', 'someone', '’', 's', 'trying', 'to', 'uh', 'take', 'the', 'cookies', '.'], ['A', 'little', 'kid', 'maybe', '?'], ['Um', ',', 'well', ',', 'I', 'guess', 'they', '’', 're', 'trying', 'to', 'get', 'the', 'cookies', 'right', '?'], ['They', '’', 're', 'they', '’', 're', 'kind', 'of', 'standing', 'on', 'something', ',', 'like', 'a', 'chair', 'maybe', '?'], ['Yeah', ',', 'and', 'they', 'look', ',', 'um', 'curious', ',', 'like', 'they', 'want', 'to', 'eat', 'them', '.'], ['Oh', ',', 'um', ',', 'the', 'woman', 'she', '’', 's', ',', 'uh', ',', 'looking', 'at', 'the', 'kid', ',', 'right', '?'], ['I', 'think', 'maybe', 'she', '’', 's', ',', 'um', ',', 'telling', 'them', 'to', 'stop', '?'], ['She', 'looks', 'maybe', 'worried', '?'], ['Or', 'mad', '?'], ['I', '’', 'm', 'not', 'sure', 'Hmm', ',', 'I', '’', 'm', 'not', 'sure', ',', 'maybe', 'she', '’', 's', ',', 'uh', ',', 'angry', 'because', 'the', 'kid', 'is', 'taking', 'the', 'cookies', 'without', 'asking', '?'], ['Or', 'maybe', ',', 'uh', ',', 'she', '’', 's', 'just', ',', 'um', ',', 'worried', 'like', ',', 'the', 'kid', 'could', 'get', 'in', 'trouble', '?'], ['I', 'can', '’', 't', 'remember', 'if', 'if', 'the', 'kid', 'is', 'supposed', 'to', 'be', 'eating', 'those']]\n",
            "Filler Word Counts:\n",
            "uh: 5\n",
            "um: 6\n",
            "like: 4\n",
            "well: 1\n",
            "maybe: 6\n",
            "just: 1\n",
            "right: 2\n",
            "yeah: 1\n",
            "hmm: 1\n",
            "\n",
            "Top 10 Frequent Words (excluding stop words):\n",
            "um: 6\n",
            "maybe: 6\n",
            "uh: 5\n",
            "kid: 5\n",
            "cookies: 4\n",
            "like: 4\n",
            "think: 2\n",
            "looks: 2\n",
            "trying: 2\n",
            "get: 2\n",
            "\n",
            "POS Tagging:\n",
            "[[('Uh', 'NNP'), ('okay', 'MD'), ('so', 'RB'), ('I', 'PRP'), ('think', 'VBP'), ('it', 'PRP'), ('’', 'VBZ'), ('s', 'VBZ'), ('a', 'DT'), ('kitchen', 'NN'), ('.', '.')], [('There', 'EX'), ('’', 'VBZ'), ('s', 'NN'), ('um', 'JJ'), ('some', 'DT'), ('cookies', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('counter', 'NN'), ('.', '.')], [('I', 'PRP'), ('don', 'VBP'), ('’', 'JJ'), ('t', 'NN'), ('know', 'VBP'), (',', ','), ('it', 'PRP'), ('looks', 'VBZ'), ('like', 'IN'), ('someone', 'NN'), ('’', 'NNP'), ('s', 'VBZ'), ('trying', 'VBG'), ('to', 'TO'), ('uh', 'VB'), ('take', 'VB'), ('the', 'DT'), ('cookies', 'NNS'), ('.', '.')], [('A', 'DT'), ('little', 'JJ'), ('kid', 'NN'), ('maybe', 'RB'), ('?', '.')], [('Um', 'NN'), (',', ','), ('well', 'RB'), (',', ','), ('I', 'PRP'), ('guess', 'VBP'), ('they', 'PRP'), ('’', 'VBP'), ('re', 'VB'), ('trying', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('the', 'DT'), ('cookies', 'NNS'), ('right', 'RB'), ('?', '.')], [('They', 'PRP'), ('’', 'VBP'), ('re', 'VBP'), ('they', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('kind', 'NN'), ('of', 'IN'), ('standing', 'VBG'), ('on', 'IN'), ('something', 'NN'), (',', ','), ('like', 'IN'), ('a', 'DT'), ('chair', 'NN'), ('maybe', 'RB'), ('?', '.')], [('Yeah', 'UH'), (',', ','), ('and', 'CC'), ('they', 'PRP'), ('look', 'VBP'), (',', ','), ('um', 'JJ'), ('curious', 'JJ'), (',', ','), ('like', 'IN'), ('they', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('eat', 'VB'), ('them', 'PRP'), ('.', '.')], [('Oh', 'UH'), (',', ','), ('um', 'JJ'), (',', ','), ('the', 'DT'), ('woman', 'NN'), ('she', 'PRP'), ('’', 'VBD'), ('s', 'NN'), (',', ','), ('uh', 'UH'), (',', ','), ('looking', 'VBG'), ('at', 'IN'), ('the', 'DT'), ('kid', 'NN'), (',', ','), ('right', 'RB'), ('?', '.')], [('I', 'PRP'), ('think', 'VBP'), ('maybe', 'RB'), ('she', 'PRP'), ('’', 'VBZ'), ('s', 'NN'), (',', ','), ('um', 'JJ'), (',', ','), ('telling', 'VBG'), ('them', 'PRP'), ('to', 'TO'), ('stop', 'VB'), ('?', '.')], [('She', 'PRP'), ('looks', 'VBZ'), ('maybe', 'RB'), ('worried', 'VBN'), ('?', '.')], [('Or', 'CC'), ('mad', 'VB'), ('?', '.')], [('I', 'PRP'), ('’', 'VBP'), ('m', 'MD'), ('not', 'RB'), ('sure', 'JJ'), ('Hmm', 'NNP'), (',', ','), ('I', 'PRP'), ('’', 'VBP'), ('m', 'MD'), ('not', 'RB'), ('sure', 'JJ'), (',', ','), ('maybe', 'RB'), ('she', 'PRP'), ('’', 'VBZ'), ('s', 'NN'), (',', ','), ('uh', 'UH'), (',', ','), ('angry', 'JJ'), ('because', 'IN'), ('the', 'DT'), ('kid', 'NN'), ('is', 'VBZ'), ('taking', 'VBG'), ('the', 'DT'), ('cookies', 'NNS'), ('without', 'IN'), ('asking', 'VBG'), ('?', '.')], [('Or', 'CC'), ('maybe', 'RB'), (',', ','), ('uh', 'UH'), (',', ','), ('she', 'PRP'), ('’', 'VBD'), ('s', 'NN'), ('just', 'RB'), (',', ','), ('um', 'JJ'), (',', ','), ('worried', 'JJ'), ('like', 'IN'), (',', ','), ('the', 'DT'), ('kid', 'NN'), ('could', 'MD'), ('get', 'VB'), ('in', 'IN'), ('trouble', 'NN'), ('?', '.')], [('I', 'PRP'), ('can', 'MD'), ('’', 'VB'), ('t', 'JJ'), ('remember', 'VB'), ('if', 'IN'), ('if', 'IN'), ('the', 'DT'), ('kid', 'NN'), ('is', 'VBZ'), ('supposed', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('eating', 'VBG'), ('those', 'DT')]]\n",
            "\n",
            "POS Tagging (Filler Words Removed):\n",
            "[[('okay', 'IN'), ('so', 'RB'), ('I', 'PRP'), ('think', 'VBP'), ('it', 'PRP'), ('’', 'VBZ'), ('s', 'VBZ'), ('a', 'DT'), ('kitchen', 'NN'), ('.', '.')], [('There', 'EX'), ('’', 'NNP'), ('s', 'VBD'), ('some', 'DT'), ('cookies', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('counter', 'NN'), ('.', '.')], [('I', 'PRP'), ('don', 'VBP'), ('’', 'JJ'), ('t', 'NN'), ('know', 'VBP'), (',', ','), ('it', 'PRP'), ('looks', 'VBZ'), ('someone', 'NN'), ('’', 'NNP'), ('s', 'VBZ'), ('trying', 'VBG'), ('to', 'TO'), ('take', 'VB'), ('the', 'DT'), ('cookies', 'NNS'), ('.', '.')], [('A', 'DT'), ('little', 'JJ'), ('kid', 'NN'), ('?', '.')], [(',', ','), (',', ','), ('I', 'PRP'), ('guess', 'VBP'), ('they', 'PRP'), ('’', 'VBP'), ('re', 'VB'), ('trying', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('the', 'DT'), ('cookies', 'NNS'), ('?', '.')], [('They', 'PRP'), ('’', 'VBP'), ('re', 'VBP'), ('they', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('kind', 'NN'), ('of', 'IN'), ('standing', 'VBG'), ('on', 'IN'), ('something', 'NN'), (',', ','), ('a', 'DT'), ('chair', 'NN'), ('?', '.')], [(',', ','), ('and', 'CC'), ('they', 'PRP'), ('look', 'VBP'), (',', ','), ('curious', 'JJ'), (',', ','), ('they', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('eat', 'VB'), ('them', 'PRP'), ('.', '.')], [('Oh', 'UH'), (',', ','), (',', ','), ('the', 'DT'), ('woman', 'NN'), ('she', 'PRP'), ('’', 'VBD'), ('s', 'NN'), (',', ','), (',', ','), ('looking', 'VBG'), ('at', 'IN'), ('the', 'DT'), ('kid', 'NN'), (',', ','), ('?', '.')], [('I', 'PRP'), ('think', 'VBP'), ('she', 'PRP'), ('’', 'VBZ'), ('s', 'NN'), (',', ','), (',', ','), ('telling', 'VBG'), ('them', 'PRP'), ('to', 'TO'), ('stop', 'VB'), ('?', '.')], [('She', 'PRP'), ('looks', 'VBZ'), ('worried', 'VBN'), ('?', '.')], [('Or', 'CC'), ('mad', 'VB'), ('?', '.')], [('I', 'PRP'), ('’', 'VBP'), ('m', 'MD'), ('not', 'RB'), ('sure', 'JJ'), (',', ','), ('I', 'PRP'), ('’', 'VBP'), ('m', 'MD'), ('not', 'RB'), ('sure', 'JJ'), (',', ','), ('she', 'PRP'), ('’', 'VBD'), ('s', 'NN'), (',', ','), (',', ','), ('angry', 'JJ'), ('because', 'IN'), ('the', 'DT'), ('kid', 'NN'), ('is', 'VBZ'), ('taking', 'VBG'), ('the', 'DT'), ('cookies', 'NNS'), ('without', 'IN'), ('asking', 'VBG'), ('?', '.')], [('Or', 'CC'), (',', ','), (',', ','), ('she', 'PRP'), ('’', 'VBD'), ('s', 'NN'), (',', ','), (',', ','), ('worried', 'VBD'), (',', ','), ('the', 'DT'), ('kid', 'NN'), ('could', 'MD'), ('get', 'VB'), ('in', 'IN'), ('trouble', 'NN'), ('?', '.')], [('I', 'PRP'), ('can', 'MD'), ('’', 'VB'), ('t', 'JJ'), ('remember', 'VB'), ('if', 'IN'), ('if', 'IN'), ('the', 'DT'), ('kid', 'NN'), ('is', 'VBZ'), ('supposed', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('eating', 'VBG'), ('those', 'DT')]]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the transcript\n",
        "transcript = \"\"\"\n",
        "Uh okay so I think it’s a kitchen. There’s um some cookies on the counter.\n",
        "I don’t know, it looks like someone’s trying to uh take the cookies. A little kid maybe?\n",
        "Um, well, I guess they’re trying to get the cookies right? They’re they’re kind of standing on\n",
        "something, like a chair maybe? Yeah, and they look, um curious, like they want to eat them.\n",
        "Oh, um, the woman she’s, uh, looking at the kid, right? I think maybe she’s, um, telling them to stop?\n",
        "She looks maybe worried? Or mad? I’m not sure\n",
        "Hmm, I’m not sure, maybe she’s, uh, angry because the kid is taking the cookies without asking? Or maybe, uh, she’s\n",
        "just, um, worried like, the kid could get in trouble? I can’t remember if if the kid is supposed to be eating those\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize the transcript first by sentence then by word since futher processing does not perform well on large chunks of text\n",
        "sents = [ nltk.word_tokenize(s) for s in nltk.sent_tokenize(transcript) ]\n",
        "\n",
        "print(sents)\n",
        "\n",
        "# Part-of-Speech tagging\n",
        "tagged_tokens = nltk.pos_tag_sents(sents)\n",
        "\n",
        "# List of common filler words (you can expand this list as needed)\n",
        "filler_words = ['uh', 'um', 'like', 'well', 'maybe', 'just', 'right', 'yeah', 'hmm']\n",
        "\n",
        "\n",
        "# Remove filler words from transcript\n",
        "for word in filler_words:\n",
        "    transcript = re.sub(r'\\b' + word + r'\\b', '', transcript, flags=re.IGNORECASE)\n",
        "\n",
        "# Tokenize the transcript first by sentence then by word since futher processing does not perform well on large chunks of text\n",
        "sents_filler_removed = [ nltk.word_tokenize(s) for s in nltk.sent_tokenize(transcript) ]\n",
        "\n",
        "# Part-of-Speech tagging\n",
        "tagged_tokens_filler_removed = nltk.pos_tag_sents(sents_filler_removed)\n",
        "\n",
        "# Count the filler words\n",
        "filler_counts = {filler: sum(1 for sentence in sents for token in sentence if token.lower() == filler) for filler in filler_words}\n",
        "\n",
        "# Word frequency (excluding stop words)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word.lower() for sentence in sents for word in sentence if word.lower() not in stop_words and word.isalnum()]\n",
        "\n",
        "# Frequency analysis of non-stop words\n",
        "word_freq = Counter(filtered_tokens)\n",
        "\n",
        "# Output the results\n",
        "print(\"Filler Word Counts:\")\n",
        "for word, count in filler_counts.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\nTop 10 Frequent Words (excluding stop words):\")\n",
        "for word, count in word_freq.most_common(10):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\nPOS Tagging:\")\n",
        "print(tagged_tokens)\n",
        "\n",
        "print(\"\\nPOS Tagging (Filler Words Removed):\")\n",
        "print(tagged_tokens_filler_removed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trying with spaCY Tagging instead of NLTK\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Load the spaCy language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the transcript\n",
        "transcript = \"\"\"\n",
        "Uh okay so I think it’s a kitchen. There’s um some cookies on the counter.\n",
        "I don’t know, it looks like someone’s trying to uh take the cookies. A little kid maybe?\n",
        "Um, well, I guess they’re trying to get the cookies right? They’re they’re kind of standing on\n",
        "something, like a chair maybe? Yeah, and they look, um curious, like they want to eat them.\n",
        "Oh, um, the woman she’s, uh, looking at the kid, right? I think maybe she’s, um, telling them to stop?\n",
        "She looks maybe worried? Or mad? I’m not sure\n",
        "Hmm, I’m not sure, maybe she’s, uh, angry because the kid is taking the cookies without asking? Or maybe, uh, she’s\n",
        "just, um, worried like, the kid could get in trouble? I can’t remember if if the kid is supposed to be eating those\n",
        "\"\"\"\n",
        "\n",
        "# List of common filler words\n",
        "filler_words = ['uh', 'um', 'like', 'well', 'maybe', 'just', 'right', 'yeah', 'hmm']\n",
        "\n",
        "# Process the transcript with spaCy\n",
        "doc = nlp(transcript)\n",
        "\n",
        "# Filler word removal\n",
        "cleaned_transcript = \" \".join([token.text for token in doc if token.text.lower() not in filler_words])\n",
        "\n",
        "# Re-process the cleaned transcript with spaCy\n",
        "doc_cleaned = nlp(cleaned_transcript)\n",
        "\n",
        "# Count the filler words in the original transcript\n",
        "filler_counts = Counter([token.text.lower() for token in doc if token.text.lower() in filler_words])\n",
        "\n",
        "# Word frequency (excluding stop words)\n",
        "filtered_tokens = [token.text.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
        "word_freq = Counter(filtered_tokens)\n",
        "\n",
        "# POS tagging\n",
        "tagged_tokens = [(token.text, token.pos_) for token in doc]\n",
        "tagged_tokens_cleaned = [(token.text, token.pos_) for token in doc_cleaned]\n",
        "\n",
        "# Output the results\n",
        "print(\"Filler Word Counts:\")\n",
        "for word, count in filler_counts.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\nTop 10 Frequent Words (excluding stop words):\")\n",
        "for word, count in word_freq.most_common(10):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\nPOS Tagging (Original Transcript):\")\n",
        "print(tagged_tokens)\n",
        "\n",
        "print(\"\\nPOS Tagging (Filler Words Removed):\")\n",
        "print(tagged_tokens_cleaned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iPtNPGgchkO",
        "outputId": "4b862f83-172a-4b08-e304-0c3c47019068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filler Word Counts:\n",
            "uh: 5\n",
            "um: 6\n",
            "like: 4\n",
            "maybe: 6\n",
            "well: 1\n",
            "right: 2\n",
            "yeah: 1\n",
            "hmm: 1\n",
            "just: 1\n",
            "\n",
            "Top 10 Frequent Words (excluding stop words):\n",
            "um: 6\n",
            "maybe: 6\n",
            "uh: 5\n",
            "kid: 5\n",
            "cookies: 4\n",
            "like: 4\n",
            "think: 2\n",
            "looks: 2\n",
            "trying: 2\n",
            "right: 2\n",
            "\n",
            "POS Tagging (Original Transcript):\n",
            "[('\\n', 'SPACE'), ('Uh', 'INTJ'), ('okay', 'INTJ'), ('so', 'ADV'), ('I', 'PRON'), ('think', 'VERB'), ('it', 'PRON'), ('’s', 'VERB'), ('a', 'DET'), ('kitchen', 'NOUN'), ('.', 'PUNCT'), ('There', 'PRON'), ('’s', 'VERB'), ('um', 'INTJ'), ('some', 'DET'), ('cookies', 'NOUN'), ('on', 'ADP'), ('the', 'DET'), ('counter', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('I', 'PRON'), ('do', 'AUX'), ('n’t', 'PART'), ('know', 'VERB'), (',', 'PUNCT'), ('it', 'PRON'), ('looks', 'VERB'), ('like', 'SCONJ'), ('someone', 'PRON'), ('’s', 'AUX'), ('trying', 'VERB'), ('to', 'PART'), ('uh', 'INTJ'), ('take', 'VERB'), ('the', 'DET'), ('cookies', 'NOUN'), ('.', 'PUNCT'), ('A', 'DET'), ('little', 'ADJ'), ('kid', 'NOUN'), ('maybe', 'ADV'), ('?', 'PUNCT'), ('\\n', 'SPACE'), ('Um', 'INTJ'), (',', 'PUNCT'), ('well', 'INTJ'), (',', 'PUNCT'), ('I', 'PRON'), ('guess', 'VERB'), ('they', 'PRON'), ('’re', 'AUX'), ('trying', 'VERB'), ('to', 'PART'), ('get', 'VERB'), ('the', 'DET'), ('cookies', 'NOUN'), ('right', 'ADJ'), ('?', 'PUNCT'), ('They', 'PRON'), ('’re', 'VERB'), ('they', 'PRON'), ('’re', 'AUX'), ('kind', 'ADV'), ('of', 'ADV'), ('standing', 'VERB'), ('on', 'ADP'), ('\\n', 'SPACE'), ('something', 'PRON'), (',', 'PUNCT'), ('like', 'ADP'), ('a', 'DET'), ('chair', 'NOUN'), ('maybe', 'ADV'), ('?', 'PUNCT'), ('Yeah', 'INTJ'), (',', 'PUNCT'), ('and', 'CCONJ'), ('they', 'PRON'), ('look', 'VERB'), (',', 'PUNCT'), ('um', 'INTJ'), ('curious', 'ADJ'), (',', 'PUNCT'), ('like', 'SCONJ'), ('they', 'PRON'), ('want', 'VERB'), ('to', 'PART'), ('eat', 'VERB'), ('them', 'PRON'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('Oh', 'INTJ'), (',', 'PUNCT'), ('um', 'INTJ'), (',', 'PUNCT'), ('the', 'DET'), ('woman', 'NOUN'), ('she', 'PRON'), ('’s', 'VERB'), (',', 'PUNCT'), ('uh', 'INTJ'), (',', 'PUNCT'), ('looking', 'VERB'), ('at', 'ADP'), ('the', 'DET'), ('kid', 'NOUN'), (',', 'PUNCT'), ('right', 'ADJ'), ('?', 'PUNCT'), ('I', 'PRON'), ('think', 'VERB'), ('maybe', 'ADV'), ('she', 'PRON'), ('’s', 'VERB'), (',', 'PUNCT'), ('um', 'INTJ'), (',', 'PUNCT'), ('telling', 'VERB'), ('them', 'PRON'), ('to', 'PART'), ('stop', 'VERB'), ('?', 'PUNCT'), ('\\n', 'SPACE'), ('She', 'PRON'), ('looks', 'VERB'), ('maybe', 'ADV'), ('worried', 'ADJ'), ('?', 'PUNCT'), ('Or', 'CCONJ'), ('mad', 'ADJ'), ('?', 'PUNCT'), ('I', 'PRON'), ('’m', 'VERB'), ('not', 'PART'), ('sure', 'ADJ'), ('\\n', 'SPACE'), ('Hmm', 'PROPN'), (',', 'PUNCT'), ('I', 'PRON'), ('’m', 'VERB'), ('not', 'PART'), ('sure', 'ADJ'), (',', 'PUNCT'), ('maybe', 'ADV'), ('she', 'PRON'), ('’s', 'VERB'), (',', 'PUNCT'), ('uh', 'INTJ'), (',', 'PUNCT'), ('angry', 'ADJ'), ('because', 'SCONJ'), ('the', 'DET'), ('kid', 'NOUN'), ('is', 'AUX'), ('taking', 'VERB'), ('the', 'DET'), ('cookies', 'NOUN'), ('without', 'ADP'), ('asking', 'VERB'), ('?', 'PUNCT'), ('Or', 'CCONJ'), ('maybe', 'ADV'), (',', 'PUNCT'), ('uh', 'INTJ'), (',', 'PUNCT'), ('she', 'PRON'), ('’s', 'VERB'), ('\\n', 'SPACE'), ('just', 'ADV'), (',', 'PUNCT'), ('um', 'INTJ'), (',', 'PUNCT'), ('worried', 'ADJ'), ('like', 'ADP'), (',', 'PUNCT'), ('the', 'DET'), ('kid', 'NOUN'), ('could', 'AUX'), ('get', 'VERB'), ('in', 'ADP'), ('trouble', 'NOUN'), ('?', 'PUNCT'), ('I', 'PRON'), ('ca', 'AUX'), ('n’t', 'PART'), ('remember', 'VERB'), ('if', 'SCONJ'), ('if', 'SCONJ'), ('the', 'DET'), ('kid', 'NOUN'), ('is', 'AUX'), ('supposed', 'VERB'), ('to', 'PART'), ('be', 'AUX'), ('eating', 'VERB'), ('those', 'PRON'), ('\\n', 'SPACE')]\n",
            "\n",
            "POS Tagging (Filler Words Removed):\n",
            "[('\\n ', 'SPACE'), ('okay', 'INTJ'), ('so', 'ADV'), ('I', 'PRON'), ('think', 'VERB'), ('it', 'PRON'), ('’s', 'VERB'), ('a', 'DET'), ('kitchen', 'NOUN'), ('.', 'PUNCT'), ('There', 'PRON'), ('’s', 'VERB'), ('some', 'DET'), ('cookies', 'NOUN'), ('on', 'ADP'), ('the', 'DET'), ('counter', 'NOUN'), ('.', 'PUNCT'), ('\\n ', 'SPACE'), ('I', 'PRON'), ('do', 'AUX'), ('n’t', 'PRON'), ('know', 'VERB'), (',', 'PUNCT'), ('it', 'PRON'), ('looks', 'VERB'), ('someone', 'PRON'), ('’s', 'AUX'), ('trying', 'VERB'), ('to', 'PART'), ('take', 'VERB'), ('the', 'DET'), ('cookies', 'NOUN'), ('.', 'PUNCT'), ('A', 'DET'), ('little', 'ADJ'), ('kid', 'NOUN'), ('?', 'PUNCT'), ('\\n ', 'SPACE'), (',', 'PUNCT'), (',', 'PUNCT'), ('I', 'PRON'), ('guess', 'VERB'), ('they', 'PRON'), ('’re', 'AUX'), ('trying', 'VERB'), ('to', 'PART'), ('get', 'VERB'), ('the', 'DET'), ('cookies', 'NOUN'), ('?', 'PUNCT'), ('They', 'PRON'), ('’re', 'VERB'), ('they', 'PRON'), ('’re', 'AUX'), ('kind', 'ADV'), ('of', 'ADV'), ('standing', 'VERB'), ('on', 'ADP'), ('\\n ', 'SPACE'), ('something', 'PRON'), (',', 'PUNCT'), ('a', 'DET'), ('chair', 'NOUN'), ('?', 'PUNCT'), (',', 'PUNCT'), ('and', 'CCONJ'), ('they', 'PRON'), ('look', 'VERB'), (',', 'PUNCT'), ('curious', 'ADJ'), (',', 'PUNCT'), ('they', 'PRON'), ('want', 'VERB'), ('to', 'PART'), ('eat', 'VERB'), ('them', 'PRON'), ('.', 'PUNCT'), ('\\n ', 'SPACE'), ('Oh', 'INTJ'), (',', 'PUNCT'), (',', 'PUNCT'), ('the', 'DET'), ('woman', 'NOUN'), ('she', 'PRON'), ('’s', 'VERB'), (',', 'PUNCT'), (',', 'PUNCT'), ('looking', 'VERB'), ('at', 'ADP'), ('the', 'DET'), ('kid', 'NOUN'), (',', 'PUNCT'), ('?', 'PUNCT'), ('I', 'PRON'), ('think', 'VERB'), ('she', 'PRON'), ('’s', 'VERB'), (',', 'PUNCT'), (',', 'PUNCT'), ('telling', 'VERB'), ('them', 'PRON'), ('to', 'PART'), ('stop', 'VERB'), ('?', 'PUNCT'), ('\\n ', 'SPACE'), ('She', 'PRON'), ('looks', 'VERB'), ('worried', 'ADJ'), ('?', 'PUNCT'), ('Or', 'CCONJ'), ('mad', 'ADJ'), ('?', 'PUNCT'), ('I', 'PRON'), ('’', 'VERB'), ('m', 'VERB'), ('not', 'PART'), ('sure', 'ADJ'), ('\\n ', 'SPACE'), (',', 'PUNCT'), ('I', 'PRON'), ('’', 'VERB'), ('m', 'VERB'), ('not', 'PART'), ('sure', 'ADJ'), (',', 'PUNCT'), ('she', 'PRON'), ('’s', 'VERB'), (',', 'PUNCT'), (',', 'PUNCT'), ('angry', 'ADJ'), ('because', 'SCONJ'), ('the', 'DET'), ('kid', 'NOUN'), ('is', 'AUX'), ('taking', 'VERB'), ('the', 'DET'), ('cookies', 'NOUN'), ('without', 'ADP'), ('asking', 'VERB'), ('?', 'PUNCT'), ('Or', 'CCONJ'), (',', 'PUNCT'), (',', 'PUNCT'), ('she', 'PRON'), ('’s', 'VERB'), ('\\n ', 'SPACE'), (',', 'PUNCT'), (',', 'PUNCT'), ('worried', 'ADJ'), (',', 'PUNCT'), ('the', 'DET'), ('kid', 'NOUN'), ('could', 'AUX'), ('get', 'VERB'), ('in', 'ADP'), ('trouble', 'NOUN'), ('?', 'PUNCT'), ('I', 'PRON'), ('ca', 'AUX'), ('n’t', 'PRON'), ('remember', 'VERB'), ('if', 'SCONJ'), ('if', 'SCONJ'), ('the', 'DET'), ('kid', 'NOUN'), ('is', 'AUX'), ('supposed', 'VERB'), ('to', 'PART'), ('be', 'AUX'), ('eating', 'VERB'), ('those', 'PRON'), ('\\n', 'SPACE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCY worked better so will use that for further analysis"
      ],
      "metadata": {
        "id": "2Ed6jhMdmAOw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WOu7HpV9l-r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load the small English model for spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def lexical_content_with_pos_analysis_spacy(text):\n",
        "    \"\"\"\n",
        "    Analyzes lexical content with part-of-speech tagging, processing sentence by sentence using spaCy.\n",
        "    \"\"\"\n",
        "    # Process the text with spaCy NLP pipeline\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Initialize counters\n",
        "    total_tokens = 0\n",
        "    total_nouns = 0\n",
        "    total_verbs = 0\n",
        "    total_filler_words = 0\n",
        "    total_open_class_words = 0\n",
        "    total_closed_class_words = 0\n",
        "\n",
        "    # Process each sentence\n",
        "    for sentence in doc.sents:\n",
        "        # Tokenize and apply POS tagging\n",
        "        tokens = [token.text for token in sentence]\n",
        "        pos_tags = [(token.text, token.pos_) for token in sentence]\n",
        "\n",
        "        #print(pos_tags)\n",
        "\n",
        "        # Update total token count\n",
        "        total_tokens += len(tokens)\n",
        "\n",
        "        # Count nouns and verbs based on POS tags\n",
        "        noun_tags = {\"NOUN\", \"PROPN\"}  # Nouns (common, proper)\n",
        "        verb_tags = {\"VERB\", \"AUX\"}  # Verbs\n",
        "        filler_words_tags = {\"INTJ\"}\n",
        "\n",
        "        nouns = [word for word, tag in pos_tags if tag in noun_tags]\n",
        "        print(\"nouns: \" + \", \".join(nouns))\n",
        "        verbs = [word for word, tag in pos_tags if tag in verb_tags]\n",
        "        print(\"verbs: \" + \", \".join(verbs))\n",
        "        filler_words = [word for word, tag in pos_tags if tag in filler_words_tags]\n",
        "        print(\"filler_words: \" + \", \".join(filler_words))\n",
        "\n",
        "        # Open-class words: nouns, verbs, adjectives, adverbs\n",
        "        open_class_tags = noun_tags | {\"VERB\", \"ADJ\", \"ADV\"}\n",
        "        open_class_words = [word for word, tag in pos_tags if tag in open_class_tags]\n",
        "        print(\"open_class_words: \" + \", \".join(open_class_words))\n",
        "\n",
        "        # Closed-class words: determiners, prepositions, conjunctions, pronouns, etc.\n",
        "        closed_class_words = [word for word, tag in pos_tags if word.isalpha() and word not in open_class_words]\n",
        "        print(\"closed_class_words: \" + \", \".join(closed_class_words))\n",
        "\n",
        "        # Update totals\n",
        "        total_nouns += len(nouns)\n",
        "        total_verbs += len(verbs)\n",
        "        total_filler_words += len(filler_words)\n",
        "        total_open_class_words += len(open_class_words)\n",
        "        total_closed_class_words += len(closed_class_words)\n",
        "\n",
        "    # Calculate metrics\n",
        "    open_closed_ratio = (\n",
        "        total_open_class_words / total_closed_class_words if total_closed_class_words else 0\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"Total Sentences\": len(list(doc.sents)),\n",
        "        \"Total Tokens\": total_tokens,\n",
        "        \"Frequency of Nouns\": total_nouns,\n",
        "        \"Frequency of Verbs and auxillary verbs\": total_verbs,\n",
        "        \"Frequency of Filler Words\": total_filler_words,\n",
        "        \"Open-Class Words\": total_open_class_words,\n",
        "        \"Closed-Class Words\": total_closed_class_words,\n",
        "        \"Open/Closed Class Ratio\": round(open_closed_ratio, 3),\n",
        "    }\n",
        "\n",
        "# Sample text\n",
        "speech_sample = \"\"\"\n",
        "The boy is standing on a stool reaching for the cookie jar.\n",
        "The stool is tipping, and his sister is trying to hold it steady.\n",
        "\"\"\"\n",
        "\n",
        "# Run Lexical Content Analysis with POS Tagging using spaCy\n",
        "lexical_pos_results_spacy = lexical_content_with_pos_analysis_spacy(transcript)\n",
        "print(\"Lexical Content Analysis with POS Tagging (Sentence-by-Sentence) using spaCy:\")\n",
        "for metric, value in lexical_pos_results_spacy.items():\n",
        "    print(f\"  {metric}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRVAeYq6fCJn",
        "outputId": "6264c188-3590-47a5-90d8-41c413cff748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nouns: \n",
            "verbs: \n",
            "filler_words: \n",
            "open_class_words: \n",
            "closed_class_words: \n",
            "nouns: \n",
            "verbs: \n",
            "filler_words: Uh\n",
            "open_class_words: \n",
            "closed_class_words: Uh\n",
            "nouns: \n",
            "verbs: \n",
            "filler_words: okay\n",
            "open_class_words: \n",
            "closed_class_words: okay\n",
            "nouns: kitchen\n",
            "verbs: think, ’s\n",
            "filler_words: \n",
            "open_class_words: so, think, ’s, kitchen\n",
            "closed_class_words: I, it, a\n",
            "nouns: cookies, counter\n",
            "verbs: ’s\n",
            "filler_words: um\n",
            "open_class_words: ’s, cookies, counter\n",
            "closed_class_words: There, um, some, on, the\n",
            "nouns: cookies\n",
            "verbs: do, know, looks, ’s, trying, take\n",
            "filler_words: uh\n",
            "open_class_words: know, looks, trying, take, cookies\n",
            "closed_class_words: I, do, it, like, someone, to, uh, the\n",
            "nouns: kid, cookies\n",
            "verbs: guess, ’re, trying, get\n",
            "filler_words: Um, well\n",
            "open_class_words: little, kid, maybe, guess, trying, get, cookies, right\n",
            "closed_class_words: A, Um, well, I, they, to, the\n",
            "nouns: chair\n",
            "verbs: ’re, ’re, standing\n",
            "filler_words: \n",
            "open_class_words: ’re, kind, of, standing, chair, maybe\n",
            "closed_class_words: They, they, on, something, like, a\n",
            "nouns: \n",
            "verbs: \n",
            "filler_words: Yeah\n",
            "open_class_words: \n",
            "closed_class_words: Yeah\n",
            "nouns: \n",
            "verbs: look, want, eat\n",
            "filler_words: um\n",
            "open_class_words: look, curious, want, eat\n",
            "closed_class_words: and, they, um, like, they, to, them\n",
            "nouns: woman, kid\n",
            "verbs: ’s, looking\n",
            "filler_words: Oh, um, uh\n",
            "open_class_words: woman, ’s, looking, kid, right\n",
            "closed_class_words: Oh, um, the, she, uh, at, the\n",
            "nouns: \n",
            "verbs: think, ’s, telling, stop\n",
            "filler_words: um\n",
            "open_class_words: think, maybe, ’s, telling, stop\n",
            "closed_class_words: I, she, um, them, to\n",
            "nouns: \n",
            "verbs: looks\n",
            "filler_words: \n",
            "open_class_words: looks, maybe, worried\n",
            "closed_class_words: She\n",
            "nouns: \n",
            "verbs: \n",
            "filler_words: \n",
            "open_class_words: mad\n",
            "closed_class_words: Or\n",
            "nouns: Hmm, kid, cookies\n",
            "verbs: ’m, ’m, ’s, is, taking, asking\n",
            "filler_words: uh\n",
            "open_class_words: ’m, sure, Hmm, ’m, sure, maybe, ’s, angry, kid, taking, cookies, asking\n",
            "closed_class_words: I, not, I, not, she, uh, because, the, is, the, without\n",
            "nouns: kid, trouble\n",
            "verbs: ’s, could, get\n",
            "filler_words: uh, um\n",
            "open_class_words: maybe, ’s, just, worried, kid, get, trouble\n",
            "closed_class_words: Or, uh, she, um, like, the, could, in\n",
            "nouns: kid\n",
            "verbs: ca, remember, is, supposed, be, eating\n",
            "filler_words: \n",
            "open_class_words: remember, kid, supposed, eating\n",
            "closed_class_words: I, ca, if, if, the, is, to, be, those\n",
            "Lexical Content Analysis with POS Tagging (Sentence-by-Sentence) using spaCy:\n",
            "  Total Sentences: 17\n",
            "  Total Tokens: 198\n",
            "  Frequency of Nouns: 15\n",
            "  Frequency of Verbs and auxillary verbs: 41\n",
            "  Frequency of Filler Words: 15\n",
            "  Open-Class Words: 67\n",
            "  Closed-Class Words: 81\n",
            "  Open/Closed Class Ratio: 0.827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def lexical_content_with_pos_analysis_nltk(text):\n",
        "    \"\"\"\n",
        "    Analyzes lexical content with part-of-speech tagging, processing sentence by sentence using NLTK.\n",
        "    \"\"\"\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    # Initialize counters\n",
        "    total_tokens = 0\n",
        "    total_nouns = 0\n",
        "    total_verbs = 0\n",
        "    total_filler_words = 0\n",
        "    total_open_class_words = 0\n",
        "    total_closed_class_words = 0\n",
        "\n",
        "    # Define POS tag sets\n",
        "    noun_tags = {\"NN\", \"NNS\", \"NNP\", \"NNPS\"}  # Nouns (common, proper)\n",
        "    verb_tags = {\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"}  # Verbs\n",
        "    filler_words_tags = {\"UH\"}  # Interjections (filler words)\n",
        "    open_class_tags = noun_tags | verb_tags | {\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\"}  # Open-class\n",
        "    closed_class_tags = {\"DT\", \"IN\", \"CC\", \"PRP\", \"PRP$\", \"TO\", \"MD\", \"WP\", \"WP$\", \"WRB\", \"WDT\", \"EX\"}  # Closed-class\n",
        "\n",
        "    # Process each sentence\n",
        "    for sentence in sentences:\n",
        "        # Tokenize the sentence into words\n",
        "        tokens = nltk.word_tokenize(sentence)\n",
        "        pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "        # Update total token count\n",
        "        total_tokens += len(tokens)\n",
        "\n",
        "        # Count nouns and verbs based on POS tags\n",
        "        nouns = [word for word, tag in pos_tags if tag in noun_tags]\n",
        "        print(\"nouns: \" + \", \".join(nouns))\n",
        "        verbs = [word for word, tag in pos_tags if tag in verb_tags]\n",
        "        print(\"verbs: \" + \", \".join(verbs))\n",
        "        filler_words = [word for word, tag in pos_tags if tag in filler_words_tags]\n",
        "        print(\"filler_words: \" + \", \".join(filler_words))\n",
        "\n",
        "        # Open-class words\n",
        "        open_class_words = [word for word, tag in pos_tags if tag in open_class_tags]\n",
        "        print(\"open_class_words: \" + \", \".join(open_class_words))\n",
        "\n",
        "        # Closed-class words\n",
        "        closed_class_words = [word for word, tag in pos_tags if tag in closed_class_tags]\n",
        "        print(\"closed_class_words: \" + \", \".join(closed_class_words))\n",
        "\n",
        "        # Update totals\n",
        "        total_nouns += len(nouns)\n",
        "        total_verbs += len(verbs)\n",
        "        total_filler_words += len(filler_words)\n",
        "        total_open_class_words += len(open_class_words)\n",
        "        total_closed_class_words += len(closed_class_words)\n",
        "\n",
        "    # Calculate metrics\n",
        "    open_closed_ratio = (\n",
        "        total_open_class_words / total_closed_class_words if total_closed_class_words else 0\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"Total Sentences\": len(sentences),\n",
        "        \"Total Tokens\": total_tokens,\n",
        "        \"Frequency of Nouns\": total_nouns,\n",
        "        \"Frequency of Verbs and auxiliary verbs\": total_verbs,\n",
        "        \"Frequency of Filler Words\": total_filler_words,\n",
        "        \"Open-Class Words\": total_open_class_words,\n",
        "        \"Closed-Class Words\": total_closed_class_words,\n",
        "        \"Open/Closed Class Ratio\": round(open_closed_ratio, 3),\n",
        "    }\n",
        "\n",
        "# Sample text\n",
        "speech_sample = \"\"\"\n",
        "The boy is standing on a stool reaching for the cookie jar.\n",
        "The stool is tipping, and his sister is trying to hold it steady.\n",
        "\"\"\"\n",
        "\n",
        "# Run Lexical Content Analysis with POS Tagging using NLTK\n",
        "lexical_pos_results_nltk = lexical_content_with_pos_analysis_nltk(transcript)\n",
        "print(\"Lexical Content Analysis with POS Tagging (Sentence-by-Sentence) using NLTK:\")\n",
        "for metric, value in lexical_pos_results_nltk.items():\n",
        "    print(f\"  {metric}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5uwaHnPrTDA",
        "outputId": "c8d00af5-4ac4-4b85-8628-b3fea48e4b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nouns: Uh, kitchen\n",
            "verbs: think, ’, s\n",
            "filler_words: \n",
            "open_class_words: Uh, so, think, ’, s, kitchen\n",
            "closed_class_words: okay, I, it, a\n",
            "nouns: s, cookies, counter\n",
            "verbs: ’\n",
            "filler_words: \n",
            "open_class_words: ’, s, um, cookies, counter\n",
            "closed_class_words: There, some, on, the\n",
            "nouns: t, someone, ’, cookies\n",
            "verbs: don, know, looks, s, trying, uh, take\n",
            "filler_words: \n",
            "open_class_words: don, ’, t, know, looks, someone, ’, s, trying, uh, take, cookies\n",
            "closed_class_words: I, it, like, to, the\n",
            "nouns: kid\n",
            "verbs: \n",
            "filler_words: \n",
            "open_class_words: little, kid, maybe\n",
            "closed_class_words: A\n",
            "nouns: Um, cookies\n",
            "verbs: guess, ’, re, trying, get\n",
            "filler_words: \n",
            "open_class_words: Um, well, guess, ’, re, trying, get, cookies, right\n",
            "closed_class_words: I, they, to, the\n",
            "nouns: kind, something, chair\n",
            "verbs: ’, re, ’, standing\n",
            "filler_words: \n",
            "open_class_words: ’, re, ’, re, kind, standing, something, chair, maybe\n",
            "closed_class_words: They, they, of, on, like, a\n",
            "nouns: \n",
            "verbs: look, want, eat\n",
            "filler_words: Yeah\n",
            "open_class_words: look, um, curious, want, eat\n",
            "closed_class_words: and, they, like, they, to, them\n",
            "nouns: woman, s, kid\n",
            "verbs: ’, looking\n",
            "filler_words: Oh, uh\n",
            "open_class_words: um, woman, ’, s, looking, kid, right\n",
            "closed_class_words: the, she, at, the\n",
            "nouns: s\n",
            "verbs: think, ’, telling, stop\n",
            "filler_words: \n",
            "open_class_words: think, maybe, ’, s, um, telling, stop\n",
            "closed_class_words: I, she, them, to\n",
            "nouns: \n",
            "verbs: looks, worried\n",
            "filler_words: \n",
            "open_class_words: looks, maybe, worried\n",
            "closed_class_words: She\n",
            "nouns: \n",
            "verbs: mad\n",
            "filler_words: \n",
            "open_class_words: mad\n",
            "closed_class_words: Or\n",
            "nouns: Hmm, s, kid, cookies\n",
            "verbs: ’, ’, ’, is, taking, asking\n",
            "filler_words: uh\n",
            "open_class_words: ’, not, sure, Hmm, ’, not, sure, maybe, ’, s, angry, kid, is, taking, cookies, asking\n",
            "closed_class_words: I, m, I, m, she, because, the, the, without\n",
            "nouns: s, kid, trouble\n",
            "verbs: ’, get\n",
            "filler_words: uh\n",
            "open_class_words: maybe, ’, s, just, um, worried, kid, get, trouble\n",
            "closed_class_words: Or, she, like, the, could, in\n",
            "nouns: kid\n",
            "verbs: ’, remember, is, supposed, be, eating\n",
            "filler_words: \n",
            "open_class_words: ’, t, remember, kid, is, supposed, be, eating\n",
            "closed_class_words: I, can, if, if, the, to, those\n",
            "Lexical Content Analysis with POS Tagging (Sentence-by-Sentence) using NLTK:\n",
            "  Total Sentences: 14\n",
            "  Total Tokens: 203\n",
            "  Frequency of Nouns: 27\n",
            "  Frequency of Verbs and auxiliary verbs: 46\n",
            "  Frequency of Filler Words: 5\n",
            "  Open-Class Words: 100\n",
            "  Closed-Class Words: 62\n",
            "  Open/Closed Class Ratio: 1.613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy's English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def syntactic_complexity_analysis_spacy(text):\n",
        "    \"\"\"\n",
        "    Analyzes the syntactic complexity of a speech/text sample using spaCy.\n",
        "    \"\"\"\n",
        "    # Process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Mean Length of Utterance (MLU): Average number of words per sentence\n",
        "    sentences = list(doc.sents)\n",
        "    mean_length_of_utterance = sum(len([token for token in sentence if token.is_alpha]) for sentence in sentences) / len(sentences) if sentences else 0\n",
        "\n",
        "    # Approximate embedded clauses by counting subordinating conjunctions\n",
        "    subordinating_conjunctions = [\"although\", \"because\", \"since\", \"unless\", \"while\", \"if\", \"when\", \"that\", \"which\", \"who\"]\n",
        "    embedded_clauses = sum(1 for token in doc if token.text.lower() in subordinating_conjunctions)\n",
        "\n",
        "    # Verb Index: Verbs to utterances ratio\n",
        "    total_verbs = sum(1 for token in doc if token.pos_ in [\"VERB\", \"AUX\"])\n",
        "    verb_index = total_verbs / len(sentences) if sentences else 0\n",
        "\n",
        "    return {\n",
        "        \"Total Sentences\": len(sentences),\n",
        "        \"Mean Length of Utterance (MLU) (Average number of words per sentence)\": round(mean_length_of_utterance, 2),\n",
        "        \"Embedded Clauses\": embedded_clauses,\n",
        "        \"Verb Index (verbs to utterances ratio)\": round(verb_index, 2),\n",
        "    }\n",
        "\n",
        "# Sample text for analysis\n",
        "speech_sample = \"\"\"The boy is standing on a stool reaching for the cookie jar.\n",
        "The stool is tipping, and his sister is trying to hold it steady.\n",
        "\"\"\"\n",
        "\n",
        "# Run Syntactic Complexity Analysis\n",
        "syntactic_results = syntactic_complexity_analysis_spacy(speech_sample)\n",
        "print(\"\\nSyntactic Complexity Analysis (spaCy):\")\n",
        "for metric, value in syntactic_results.items():\n",
        "    print(f\"  {metric}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUfavMOI7P-Y",
        "outputId": "eb786b45-ceb9-4811-9290-36d0d04cbdf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Syntactic Complexity Analysis (spaCy):\n",
            "  Total Sentences: 2\n",
            "  Mean Length of Utterance (MLU) (Average number of words per sentence): 12.5\n",
            "  Embedded Clauses: 0\n",
            "  Verb Index (verbs to utterances ratio): 4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model for semantic tagging\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the cookie theft semantic bank of 23 words\n",
        "cookie_theft_bank = {\n",
        "    \"boy\", \"girl\", \"mother\", \"kitchen\", \"outside\", \"cookie\", \"jar\", \"stool\", \"sink\",\n",
        "    \"plate\", \"dishcloth\", \"water\", \"window\", \"cupboard\", \"dishes\", \"curtains\", \"taking\",\n",
        "    \"stealing\", \"falling\", \"drying\", \"washing\", \"spilling\", \"overflowing\"\n",
        "}\n",
        "\n",
        "def analyze_semantic_content_with_cookie_theft_bank(text, duration_seconds=None):\n",
        "    \"\"\"\n",
        "    Analyze semantic content metrics using a predefined semantic word bank.\n",
        "    \"\"\"\n",
        "    # Process text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Identify semantic units: Words from the cookie theft bank\n",
        "    semantic_units = [token.text.lower() for token in doc if token.text.lower() in cookie_theft_bank]\n",
        "    total_words = len([token for token in doc if token.is_alpha])  # Total words excluding punctuation\n",
        "\n",
        "    # Compute metrics\n",
        "    num_semantic_units = len(semantic_units)\n",
        "    idea_density = num_semantic_units / total_words if total_words else 0\n",
        "    semantic_efficiency = (\n",
        "        num_semantic_units / duration_seconds if duration_seconds and duration_seconds > 0 else None\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"Number of Semantic Units\": num_semantic_units,\n",
        "        \"Semantic Idea Density (Semantic Units / Total Words)\": round(idea_density, 2),\n",
        "        \"Semantic Efficiency (Semantic Units / Speech Duration)\": round(semantic_efficiency, 2)\n",
        "        if semantic_efficiency is not None else \"Duration not provided\",\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "speech_sample = \"The boy is climbing on the chair to reach the cookie jar while the mother is washing dishes at the sink.\"\n",
        "speech_duration = 30  # Duration in seconds\n",
        "\n",
        "semantic_metrics = analyze_semantic_content_with_cookie_theft_bank(speech_sample, speech_duration)\n",
        "print(\"\\nSemantic Content Metrics:\")\n",
        "for metric, value in semantic_metrics.items():\n",
        "    print(f\"  {metric}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p46JJ4kXbqej",
        "outputId": "39414693-3b83-4fdb-cd3c-4a1d00d6d486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Semantic Content Metrics:\n",
            "  Number of Semantic Units: 7\n",
            "  Semantic Idea Density (Semantic Units / Total Words): 0.33\n",
            "  Semantic Efficiency (Semantic Units / Speech Duration): 0.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Load spaCy model for semantic tagging\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define the cookie theft semantic bank of 23 words\n",
        "cookie_theft_bank = {\n",
        "    \"boy\", \"girl\", \"mother\", \"kitchen\", \"outside\", \"cookie\", \"jar\", \"stool\", \"sink\",\n",
        "    \"plate\", \"dishcloth\", \"water\", \"window\", \"cupboard\", \"dishes\", \"curtains\", \"taking\",\n",
        "    \"stealing\", \"falling\", \"drying\", \"washing\", \"spilling\", \"overflowing\"\n",
        "}\n",
        "\n",
        "def get_synonyms(word):\n",
        "    \"\"\"\n",
        "    Get synonyms for a given word using WordNet.\n",
        "    \"\"\"\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name().lower())\n",
        "    return synonyms\n",
        "\n",
        "# Precompute a synonym-expanded cookie theft bank\n",
        "expanded_cookie_theft_bank = set(cookie_theft_bank)\n",
        "for word in cookie_theft_bank:\n",
        "    expanded_cookie_theft_bank.update(get_synonyms(word))\n",
        "\n",
        "print(expanded_cookie_theft_bank)\n",
        "\n",
        "def analyze_semantic_content_with_cookie_theft_synonyms(text, duration_seconds=None):\n",
        "    \"\"\"\n",
        "    Analyze semantic content metrics using a predefined semantic word bank and synonyms.\n",
        "    \"\"\"\n",
        "    # Process text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Identify semantic units: Words from the synonym-expanded cookie theft bank\n",
        "    semantic_units = [\n",
        "        token.text.lower() for token in doc if token.text.lower() in expanded_cookie_theft_bank\n",
        "    ]\n",
        "    total_words = len([token for token in doc if token.is_alpha])  # Total words excluding punctuation\n",
        "\n",
        "    # Compute metrics\n",
        "    num_semantic_units = len(semantic_units)\n",
        "    idea_density = num_semantic_units / total_words if total_words else 0\n",
        "    semantic_efficiency = (\n",
        "        num_semantic_units / duration_seconds if duration_seconds and duration_seconds > 0 else None\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"Number of Semantic Units\": num_semantic_units,\n",
        "        \"Semantic Idea Density (Semantic Units / Total Words)\": round(idea_density, 2),\n",
        "        \"Semantic Efficiency (Semantic Units / Speech Duration)\": round(semantic_efficiency, 2)\n",
        "        if semantic_efficiency is not None else \"Duration not provided\",\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "speech_sample = \"The boy is climbing on the chair to grab the cookie jar while the mother is drying dishes at the sink.\"\n",
        "speech_duration = 30  # Duration in seconds\n",
        "\n",
        "# Ensure NLTK resources are available (run this once)\n",
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "semantic_metrics = analyze_semantic_content_with_cookie_theft_synonyms(speech_sample, speech_duration)\n",
        "print(\"\\nSemantic Content Metrics:\")\n",
        "for metric, value in semantic_metrics.items():\n",
        "    print(f\"  {metric}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnPvJuLV51RA",
        "outputId": "bb7df42e-3c31-4b8e-c969-46a2eeb40741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'lessen', 'pack', 'fecal_matter', 'come_down', 'subscribe_to', 'run_over', 'cup_of_tea', 'take_up', 'cooky', 'peach', 'male_child', 'slop', 'brim_over', 'curtain', 'stool', 'thievery', 'pee', 'go_under', 'well_over', 'take_on', 'cesspit', 'settle', 'accept', 'jolt', 'cupboard', 'knockout', 'hire', 'plate', 'exterior', 'direct', 'dropping', 'fetching', 'sweetheart', 'fill', 'external', 'can', 'outside', 'spill', 'submit', 'girl', 'piss', 'slump', 'take_in', 'take_a_shit', 'outdoor', 'serve_up', 'occupy', 'make', 'claim', 'talk', 'dampen', 'wash_out', 'lapse', 'aim', 'fall_off', 'fall', 'look_at', 'h2o', 'subscribe', 'strike', 'water_supply', 'dish_aerial', 'jar', 'outdoors', 'acquire', 'call_for', 'drive', 'slip', 'assume', 'return', 'windowpane', 'involve', 'extraneous', 'defecate', 'dental_plate', 'scale', 'throne', 'jounce', 'remote', 'bury', 'drapery', 'drape', 'collide', 'winning', 'light', 'irrigate', 'charter', 'beauty', 'little_girl', 'awash', 'overprotect', 'hang', 'spill_over', 'denture', 'pour_forth', 'demand', 'admit', 'urine', 'kitchen', 'collection_plate', 'ingest', 'dishcloth', 'lead', 'cesspool', 'lease', 'moisten', 'home_base', 'saucer', 'female_parent', 'dejection', 'dip', 'young_lady', 'jarful', 'rinse', 'accrue', 'spilling', 'convey', 'shit', 'pickings', 'faecal_matter', 'overrun', 'sump', 'engender', 'piddle', 'ca-ca', 'lave', 'stealing', 'falling', 'drying', 'young_woman', 'study', 'faeces', 'exact', 'curtains', 'son', 'take_aim', 'ordure', 'dish_up', 'dishrag', 'pot', 'train', 'steal', 'wash_off', 'boy', 'shed', 'pick_out', 'bm', 'disgorge', 'pass', 'crap', 'fall_down', 'closet', 'dish', 'taking', 'stunner', 'lulu', 'use_up', 'tiller', 'swallow_hole', 'take', 'ask', 'home', 'plateful', 'stealth', 'diminish', 'body_of_water', 'ravisher', 'cookie', 'biscuit', 'toilet', 'lap', 'postulate', 'beget', 'remove', 'need', 'dry', 'have', 'miss', 'take_a_crap', 'go_down', 'alfresco', 'inundated', 'thieving', 'sire', 'weewee', 'devolve', 'sink', 'shine', 'clash', 'missy', 'slide_down', 'window', 'wash', 'guide', 'smasher', 'bubble_over', 'learn', 'select', 'shell', 'take_away', 'flow', 'drop', 'launder', 'looker', 'girlfriend', 'necessitate', 'overflow', 'afloat', 'dish_out', 'rent', 'bag', 'female_child', 'conduct', 'choose', 'descend', 'potty', 'laundry', 'larceny', 'shock', 'overflowing', 'international', 'mantle', 'father', 'photographic_plate', 'withdraw', 'contract', 'flooded', 'drop_down', 'require', 'get', 'crustal_plate', 'wash_away', 'splatter', 'serve', 'dishful', 'mantrap', 'bump_around', 'washables', 'sinkhole', 'consume', 'dishes', 'dish_antenna', 'out-of-door', 'deal', 'lavation', 'bring_forth', 'fille', 'subside', 'hold', 'contain', 'bring', 'shoot', 'mother', 'pall', 'read', 'get_hold_of', 'shake_up', 'water_system', 'decrease', 'consider', 'home_plate', 'carry', 'fuss', 'feces', 'run_out', 'commode', 'lady_friend', 'water', 'away', 'adopt', 'dry_out', 'daughter', 'out_of_doors', 'film', 'engage', 'theft', 'come', 'washing', 'generate', 'precipitate', 'crapper'}\n",
            "\n",
            "Semantic Content Metrics:\n",
            "  Number of Semantic Units: 7\n",
            "  Semantic Idea Density (Semantic Units / Total Words): 0.33\n",
            "  Semantic Efficiency (Semantic Units / Speech Duration): 0.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run different Test cases with speech similar to MCI"
      ],
      "metadata": {
        "id": "vfHgw2fy1qEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "speech_1_mci = \"\"\"Uh... I was... um, thinking about going to the... uh, store,\n",
        "but then, I, uh, forgot what I... needed. I think it was something for dinner,\n",
        "um, yeah... but I, uh, just can't remember.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Run Lexical, Semantic and Syntactic Analysis. use extimate fo speech duration for now\n",
        "lexical_pos_results_1 = lexical_content_with_pos_analysis_spacy(speech_1_mci)\n",
        "semantic_metrics_mci_1 = analyze_semantic_content_with_cookie_theft_bank(speech_1_mci, duration_seconds=25)  # Assuming 25 seconds\n",
        "syntactic_results_mci_1 = syntactic_complexity_analysis_spacy(speech_1_mci)\n",
        "\n",
        "\n",
        "print(\"Lexical Content Analysis with POS Tagging (Sentence-by-Sentence) using spaCy:\")\n",
        "for metric, value in lexical_pos_results_1.items():\n",
        "    print(f\"  {metric}: {value}\")\n",
        "\n",
        "print(\"\\nSemantic Metrics:\")\n",
        "for metric, value in semantic_metrics_mci_1.items():\n",
        "    print(f\"  {metric}: {value}\")\n",
        "\n",
        "print(\"\\nSyntactic Complexity Metrics:\")\n",
        "for metric, value in syntactic_results_mci_1.items():\n",
        "    print(f\"  {metric}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_lSjRIe0ygX",
        "outputId": "b2232730-f8a1-4cc1-b393-99c2a8a51a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nouns: store\n",
            "verbs: was, thinking, going, forgot\n",
            "filler_words: Uh, um, uh, uh\n",
            "open_class_words: thinking, going, store, then, forgot, needed\n",
            "closed_class_words: Uh, I, was, um, about, to, the, uh, but, I, uh, what, I\n",
            "nouns: dinner\n",
            "verbs: think, was\n",
            "filler_words: um, yeah\n",
            "open_class_words: think, dinner\n",
            "closed_class_words: I, it, was, something, for, um, yeah\n",
            "nouns: \n",
            "verbs: ca, remember\n",
            "filler_words: uh\n",
            "open_class_words: just, remember\n",
            "closed_class_words: but, I, uh, ca\n",
            "Lexical Content Analysis with POS Tagging (Sentence-by-Sentence) using spaCy:\n",
            "  Total Sentences: 3\n",
            "  Total Tokens: 55\n",
            "  Frequency of Nouns: 2\n",
            "  Frequency of Verbs and auxillary verbs: 8\n",
            "  Frequency of Filler Words: 7\n",
            "  Open-Class Words: 10\n",
            "  Closed-Class Words: 24\n",
            "  Open/Closed Class Ratio: 0.417\n",
            "\n",
            "Semantic Metrics:\n",
            "  Number of Semantic Units: 0\n",
            "  Semantic Idea Density (Semantic Units / Total Words): 0.0\n",
            "  Semantic Efficiency (Semantic Units / Speech Duration): 0.0\n",
            "\n",
            "Syntactic Complexity Metrics:\n",
            "  Total Sentences: 3\n",
            "  Mean Length of Utterance (MLU) (Average number of words per sentence): 11.33\n",
            "  Embedded Clauses: 0\n",
            "  Verb Index (verbs to utterances ratio): 2.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nouns: 2/2\n",
        "verbs: 7/7\n",
        "filler words: 7/7"
      ],
      "metadata": {
        "id": "2CJXU85L2Dk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "speech_2_mci = \"\"\"I went to the store, and I bought some apples. Apples, you know?\n",
        "I got apples, and I thought I might make a pie. Oh, and I also got some milk.\n",
        "The milk... I got the milk to, uh, make the pie.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Run Lexical, Semantic and Syntactic Analysis. use extimate fo speech duration for now\n",
        "lexical_pos_results_2 = lexical_content_with_pos_analysis_spacy(speech_2_mci)\n",
        "semantic_metrics_mci_2 = analyze_semantic_content_with_cookie_theft_bank(speech_2_mci, duration_seconds=25)  # Assuming 25 seconds\n",
        "syntactic_results_mci_2 = syntactic_complexity_analysis_spacy(speech_2_mci)\n",
        "\n",
        "\n",
        "print(\"Lexical Content Analysis with POS Tagging (Sentence-by-Sentence) using spaCy:\")\n",
        "for metric, value in lexical_pos_results_2.items():\n",
        "    print(f\"  {metric}: {value}\")\n",
        "\n",
        "print(\"\\nSemantic Metrics:\")\n",
        "for metric, value in semantic_metrics_mci_2.items():\n",
        "    print(f\"  {metric}: {value}\")\n",
        "\n",
        "print(\"\\nSyntactic Complexity Metrics:\")\n",
        "for metric, value in syntactic_results_mci_2.items():\n",
        "    print(f\"  {metric}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xiqi96e_8qbl",
        "outputId": "41047a44-42d1-4d54-b3a1-d91c471d6da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nouns: store, apples\n",
            "verbs: went, bought\n",
            "filler_words: \n",
            "open_class_words: went, store, bought, apples\n",
            "closed_class_words: I, to, the, and, I, some\n",
            "nouns: Apples\n",
            "verbs: know\n",
            "filler_words: \n",
            "open_class_words: Apples, know\n",
            "closed_class_words: you\n",
            "nouns: apples, pie\n",
            "verbs: got, thought, might, make\n",
            "filler_words: \n",
            "open_class_words: got, apples, thought, make, pie\n",
            "closed_class_words: I, and, I, I, might, a\n",
            "nouns: milk\n",
            "verbs: got\n",
            "filler_words: Oh\n",
            "open_class_words: also, got, milk\n",
            "closed_class_words: Oh, and, I, some\n",
            "nouns: milk\n",
            "verbs: \n",
            "filler_words: \n",
            "open_class_words: milk\n",
            "closed_class_words: The\n",
            "nouns: milk, pie\n",
            "verbs: got, to, make\n",
            "filler_words: uh\n",
            "open_class_words: got, milk, make, pie\n",
            "closed_class_words: I, the, to, uh, the\n",
            "Lexical Content Analysis with POS Tagging (Sentence-by-Sentence) using spaCy:\n",
            "  Total Sentences: 6\n",
            "  Total Tokens: 57\n",
            "  Frequency of Nouns: 9\n",
            "  Frequency of Verbs: 11\n",
            "  Frequency of Filler Words: 2\n",
            "  Open-Class Words: 19\n",
            "  Closed-Class Words: 23\n",
            "  Open/Closed Class Ratio: 0.826\n",
            "\n",
            "MCI Speech 1 Semantic Metrics:\n",
            "  Number of Semantic Units: 19\n",
            "  Semantic Idea Density (Semantic Units / Total Words): 0.45\n",
            "  Semantic Efficiency (Semantic Units / Speech Duration): 0.76\n",
            "\n",
            "MCI Speech 1 Syntactic Complexity Metrics:\n",
            "  Total Sentences: 6\n",
            "  Mean Length of Utterance (MLU) (Average number of words per sentence): 7.0\n",
            "  Embedded Clauses: 0\n",
            "  Verb Index (verbs to utterances ratio): 1.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "speech_3_mci = \"\"\"I, uh, was... looking for that thing... uh, you know, the thing that,\n",
        "um, helps with... uh... I can't remember... the thing... for my, uh... uh, for my... my...\n",
        "my knees... yeah, my knees...\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Run Lexical, Semantic and Syntactic Analysis. use extimate fo speech duration for now\n",
        "lexical_pos_results_3 = lexical_content_with_pos_analysis_spacy(speech_3_mci)\n",
        "semantic_metrics_mci_3 = analyze_semantic_content_with_cookie_theft_bank(speech_3_mci, duration_seconds=25)  # Assuming 25 seconds\n",
        "syntactic_results_mci_3 = syntactic_complexity_analysis_spacy(speech_3_mci)\n",
        "\n",
        "\n",
        "print(\"Lexical Content Analysis with POS Tagging (Sentence-by-Sentence) using spaCy:\")\n",
        "for metric, value in lexical_pos_results_3.items():\n",
        "    print(f\"  {metric}: {value}\")\n",
        "\n",
        "print(\"\\nSemantic Metrics:\")\n",
        "for metric, value in semantic_metrics_mci_3.items():\n",
        "    print(f\"  {metric}: {value}\")\n",
        "\n",
        "print(\"\\nSyntactic Complexity Metrics:\")\n",
        "for metric, value in syntactic_results_mci_3.items():\n",
        "    print(f\"  {metric}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqHZL_Pn85Z8",
        "outputId": "c071cab0-aeac-4033-ac3d-edf2cce9053d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nouns: thing\n",
            "verbs: was, looking\n",
            "filler_words: uh\n",
            "open_class_words: looking, thing\n",
            "closed_class_words: I, uh, was, for, that\n",
            "nouns: thing\n",
            "verbs: know, helps\n",
            "filler_words: uh, um, uh\n",
            "open_class_words: know, thing, helps\n",
            "closed_class_words: uh, you, the, that, um, with, uh\n",
            "nouns: thing, knees\n",
            "verbs: ca, remember\n",
            "filler_words: uh, uh\n",
            "open_class_words: remember, thing, knees\n",
            "closed_class_words: I, ca, the, for, my, uh, uh, for, my, my, my\n",
            "nouns: knees\n",
            "verbs: \n",
            "filler_words: yeah\n",
            "open_class_words: knees\n",
            "closed_class_words: yeah, my\n",
            "Lexical Content Analysis with POS Tagging (Sentence-by-Sentence) using spaCy:\n",
            "  Total Sentences: 4\n",
            "  Total Tokens: 58\n",
            "  Frequency of Nouns: 5\n",
            "  Frequency of Verbs and auxillary verbs: 6\n",
            "  Frequency of Filler Words: 7\n",
            "  Open-Class Words: 9\n",
            "  Closed-Class Words: 25\n",
            "  Open/Closed Class Ratio: 0.36\n",
            "\n",
            "MCI Speech 1 Semantic Metrics:\n",
            "  Number of Semantic Units: 9\n",
            "  Semantic Idea Density (Semantic Units / Total Words): 0.26\n",
            "  Semantic Efficiency (Semantic Units / Speech Duration): 0.36\n",
            "\n",
            "MCI Speech 1 Syntactic Complexity Metrics:\n",
            "  Total Sentences: 4\n",
            "  Mean Length of Utterance (MLU) (Average number of words per sentence): 8.5\n",
            "  Embedded Clauses: 2\n",
            "  Verb Index (verbs to utterances ratio): 1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "speech_4_mci = \"\"\"I remember going to the... uh, no, wait... it was last week,\n",
        "or maybe it was... the week before? I don't know. I had, uh, a doctor's appointment,\n",
        "but I forget what... um... what happened. Oh! And the car broke down... or was it the phone?\n",
        "I can’t... remember.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Run Lexical, Semantic and Syntactic Analysis. use extimate fo speech duration for now\n",
        "lexical_pos_results_4 = lexical_content_with_pos_analysis_spacy(speech_4_mci)\n",
        "semantic_metrics_mci_4 = analyze_semantic_content_with_cookie_theft_bank(speech_4_mci, duration_seconds=25)  # Assuming 25 seconds\n",
        "syntactic_results_mci_4 = syntactic_complexity_analysis_spacy(speech_4_mci)\n",
        "\n",
        "\n",
        "print(\"Lexical Content Analysis with POS Tagging (Sentence-by-Sentence) using spaCy:\")\n",
        "for metric, value in lexical_pos_results_4.items():\n",
        "    print(f\"  {metric}: {value}\")\n",
        "\n",
        "print(\"\\nSemantic Metrics:\")\n",
        "for metric, value in semantic_metrics_mci_4.items():\n",
        "    print(f\"  {metric}: {value}\")\n",
        "\n",
        "print(\"\\nSyntactic Complexity Metrics:\")\n",
        "for metric, value in syntactic_results_mci_4.items():\n",
        "    print(f\"  {metric}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5w207DC9Gxt",
        "outputId": "cb90cab8-eeb8-4cc3-958c-31ff0c32e76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nouns: \n",
            "verbs: remember, going, wait\n",
            "filler_words: uh, no\n",
            "open_class_words: remember, going, wait\n",
            "closed_class_words: I, to, the, uh, no\n",
            "nouns: week, week\n",
            "verbs: was, was\n",
            "filler_words: \n",
            "open_class_words: last, week, maybe, week, before\n",
            "closed_class_words: it, was, or, it, was, the\n",
            "nouns: \n",
            "verbs: do, know\n",
            "filler_words: \n",
            "open_class_words: know\n",
            "closed_class_words: I, do\n",
            "nouns: doctor, appointment\n",
            "verbs: had, forget\n",
            "filler_words: uh\n",
            "open_class_words: had, doctor, appointment, forget\n",
            "closed_class_words: I, uh, a, but, I, what\n",
            "nouns: \n",
            "verbs: happened\n",
            "filler_words: um\n",
            "open_class_words: happened\n",
            "closed_class_words: um, what\n",
            "nouns: \n",
            "verbs: \n",
            "filler_words: Oh\n",
            "open_class_words: \n",
            "closed_class_words: Oh\n",
            "nouns: car, phone\n",
            "verbs: broke, was\n",
            "filler_words: \n",
            "open_class_words: car, broke, phone\n",
            "closed_class_words: And, the, down, or, was, it, the\n",
            "nouns: \n",
            "verbs: ca, remember\n",
            "filler_words: \n",
            "open_class_words: remember\n",
            "closed_class_words: I, ca\n",
            "Lexical Content Analysis with POS Tagging (Sentence-by-Sentence) using spaCy:\n",
            "  Total Sentences: 8\n",
            "  Total Tokens: 75\n",
            "  Frequency of Nouns: 6\n",
            "  Frequency of Verbs and auxillary verbs: 14\n",
            "  Frequency of Filler Words: 5\n",
            "  Open-Class Words: 18\n",
            "  Closed-Class Words: 31\n",
            "  Open/Closed Class Ratio: 0.581\n",
            "\n",
            "MCI Speech 1 Semantic Metrics:\n",
            "  Number of Semantic Units: 19\n",
            "  Semantic Idea Density (Semantic Units / Total Words): 0.39\n",
            "  Semantic Efficiency (Semantic Units / Speech Duration): 0.76\n",
            "\n",
            "MCI Speech 1 Syntactic Complexity Metrics:\n",
            "  Total Sentences: 8\n",
            "  Mean Length of Utterance (MLU) (Average number of words per sentence): 6.12\n",
            "  Embedded Clauses: 0\n",
            "  Verb Index (verbs to utterances ratio): 1.75\n"
          ]
        }
      ]
    }
  ]
}
